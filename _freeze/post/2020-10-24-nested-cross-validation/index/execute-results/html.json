{
  "hash": "f14d7e0572afa6728cb02cd7e7cecf0a",
  "result": {
    "markdown": "---\ntitle: \"Nested cross-validation\"\nauthor: \"Philip Khor\"\ndate: '2020-10-24'\nslug: []\ncategories: []\ntags: []\nsubtitle: ''\nsummary: A solution for overfitting in model selection\nauthors: []\nlastmod: '2020-10-24T13:38:22+08:00'\nfeatured: no\nprojects: []\nbibliography: references.bib\noutput: \n  html_document: \n    keep_md: true \n---\n\n\n\n\nIn [Why do cross-validation](../why-do-cross-validation/), I described cross-validation as a way of evaluating your modeling workflow from start to end to help you pick the appropriate model and avoid overfitting on your test set. No single model from the cross-validation process should actually be used as your final model[^1]; cross-validation is merely a way to evaluate how well your modeling process works on repeated samples of your data, to get a better sense of how well your modeling choice works in the real world.\n\n[^1]: The unfortunate and unintended consequence of the wonderful `GridSearchCV()` class is that data scientists think that you can predict from a grid search process, and when they are pressed about where the 'final model' from a cross-validated process comes from, they may wonder, is it an ensemble model averaged across the folds, or are the hyperparameters averaged, or ...????\n\nIf you're comparing estimator-to-estimator without tuning much, you're at pretty low risk of overestimating model performance from model selection, so long as you don't peek at your test set. Suppose I had three simple models:\n\n\n\n\n\n``` python\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import cross_val_score, KFold, GridSearchCV\nfrom sklearn.svm import SVC\n\nimport pandas as pd \n\nX, y = load_breast_cancer(return_X_y=True) \n\nnb = GaussianNB()\nlr = LogisticRegression()\nrf = RandomForestClassifier()\n\n# by default we get accuracy over 5-fold CV \nnb_scores = cross_val_score(nb, X, y, scoring=\"accuracy\")\nlr_scores = cross_val_score(lr, X, y, scoring=\"accuracy\")\nrf_scores = cross_val_score(rf, X, y, scoring=\"accuracy\")\n```\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndict(zip(\n  [\"GaussianNB\", \"LogisticRegression\", \"RandomForestClassifier\"], \n  [round(i, 3) for i in [nb_scores.mean(), lr_scores.mean(), rf_scores.mean()]]\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'GaussianNB': 0.939, 'LogisticRegression': 0.947, 'RandomForestClassifier': 0.961}\n```\n:::\n:::\n\n\nFrom the scores above, it seems that RandomForestClassifier should perform best out-of-sample.\n\n## What about hyperparameter tuning?\n\nI specifically chose these three estimators because they require minimal tuning. However, modern gradient boosting frameworks such as LightGBM and XGBoost have [a lot of hyperparameters to choose from](https://sites.google.com/view/lauraepp/parameters), and rarely do modellers have ways to guide hyperparameter choices.\n\nTaking elastic net as an example, where the regularization term is a mix of L1 and L2 penalties:\n\n$$\n\\sum_{i=1}^{n} \\big( y^{(i)} - \\hat{y}^{(i)}  \\big)^2 + \\lambda_1 \\sum^{m}_{j=1} w_{j}^2+ \\lambda_2 \\sum^{m}_{j=1} |w_j|\n$$ which is sometimes restated to establish a 'mixing parameter' $\\alpha$ (e.g. glmnet):\n\n$$\n\\sum_{i=1}^{n} \\big( y^{(i)} - \\hat{y}^{(i)}  \\big)^2 + \\lambda\\left[ \\frac {1-\\alpha}{2}\\sum^{m}_{j=1} w_{j}^2+\\alpha \\sum^{m}_{j=1} |w_j|\\right]\n$$\n\nThe choice between L1 and L2 regularization can be guided by our hypotheses of how the feature set affects the target. For instance, if many features do not affect the target, a higher mix of L1 penalty may be more appropriate, whereas if each feature contributes a little to the target, the L2 penalty would be more appropriate. However, it is not clear how to pick the regularization term $\\lambda$.\n\n## Grid Search\n\nThe standard response to this is to perform a grid search on your hyperparameters. In scikit-learn, this can be done by fitting a GridSearchCV object.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\nsvc = SVC()\nclf = GridSearchCV(svc, parameters)\nclf.fit(X, y)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=SVC(),\n             param_grid={&#x27;C&#x27;: [1, 10], &#x27;kernel&#x27;: (&#x27;linear&#x27;, &#x27;rbf&#x27;)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=SVC(),\n             param_grid={&#x27;C&#x27;: [1, 10], &#x27;kernel&#x27;: (&#x27;linear&#x27;, &#x27;rbf&#x27;)})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>\n```\n:::\n:::\n\n\nWhat does GridSearchCV do? Going to the scikit-learn documentation:\n\n> It is possible and recommended to search the hyper-parameter space for the best cross validation score.\n\nand ...\n\n> exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter\n\n\n::: {.cell}\n\n```{.python .cell-code}\nparams_df = pd.DataFrame(clf.cv_results_[\"params\"])\nresult = pd.DataFrame({key: value for key, value in clf.cv_results_.items() if key.startswith(\"split\")})\nres = pd.concat([params_df, result], axis=1)\n```\n:::\n\n\nEssentially:\n\n1.  You construct a grid of hyperparameter values:\n\n    | C   | kernel |\n    |-----|--------|\n    | 1   | linear |\n    | 1   | rbf    |\n    | 10  | linear |\n    | 10  | rbf    |\n\n2.  you perform a 5-fold cross validation, building a model on each combination of 4 of the 5 folds and evaluating the model on the remaining fold:\n\n\n    ::: {.cell}\n    ::: {.cell-output-display}\n    ```{=html}\n    <div id=\"nsgsysbnsb\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n    <style>html {\n      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n    }\n    \n    #nsgsysbnsb .gt_table {\n      display: table;\n      border-collapse: collapse;\n      margin-left: auto;\n      margin-right: auto;\n      color: #333333;\n      font-size: 16px;\n      font-weight: normal;\n      font-style: normal;\n      background-color: #FFFFFF;\n      width: auto;\n      border-top-style: solid;\n      border-top-width: 2px;\n      border-top-color: #A8A8A8;\n      border-right-style: none;\n      border-right-width: 2px;\n      border-right-color: #D3D3D3;\n      border-bottom-style: solid;\n      border-bottom-width: 2px;\n      border-bottom-color: #A8A8A8;\n      border-left-style: none;\n      border-left-width: 2px;\n      border-left-color: #D3D3D3;\n    }\n    \n    #nsgsysbnsb .gt_heading {\n      background-color: #FFFFFF;\n      text-align: center;\n      border-bottom-color: #FFFFFF;\n      border-left-style: none;\n      border-left-width: 1px;\n      border-left-color: #D3D3D3;\n      border-right-style: none;\n      border-right-width: 1px;\n      border-right-color: #D3D3D3;\n    }\n    \n    #nsgsysbnsb .gt_caption {\n      padding-top: 4px;\n      padding-bottom: 4px;\n    }\n    \n    #nsgsysbnsb .gt_title {\n      color: #333333;\n      font-size: 125%;\n      font-weight: initial;\n      padding-top: 4px;\n      padding-bottom: 4px;\n      padding-left: 5px;\n      padding-right: 5px;\n      border-bottom-color: #FFFFFF;\n      border-bottom-width: 0;\n    }\n    \n    #nsgsysbnsb .gt_subtitle {\n      color: #333333;\n      font-size: 85%;\n      font-weight: initial;\n      padding-top: 0;\n      padding-bottom: 6px;\n      padding-left: 5px;\n      padding-right: 5px;\n      border-top-color: #FFFFFF;\n      border-top-width: 0;\n    }\n    \n    #nsgsysbnsb .gt_bottom_border {\n      border-bottom-style: solid;\n      border-bottom-width: 2px;\n      border-bottom-color: #D3D3D3;\n    }\n    \n    #nsgsysbnsb .gt_col_headings {\n      border-top-style: solid;\n      border-top-width: 2px;\n      border-top-color: #D3D3D3;\n      border-bottom-style: solid;\n      border-bottom-width: 2px;\n      border-bottom-color: #D3D3D3;\n      border-left-style: none;\n      border-left-width: 1px;\n      border-left-color: #D3D3D3;\n      border-right-style: none;\n      border-right-width: 1px;\n      border-right-color: #D3D3D3;\n    }\n    \n    #nsgsysbnsb .gt_col_heading {\n      color: #333333;\n      background-color: #FFFFFF;\n      font-size: 100%;\n      font-weight: normal;\n      text-transform: inherit;\n      border-left-style: none;\n      border-left-width: 1px;\n      border-left-color: #D3D3D3;\n      border-right-style: none;\n      border-right-width: 1px;\n      border-right-color: #D3D3D3;\n      vertical-align: bottom;\n      padding-top: 5px;\n      padding-bottom: 6px;\n      padding-left: 5px;\n      padding-right: 5px;\n      overflow-x: hidden;\n    }\n    \n    #nsgsysbnsb .gt_column_spanner_outer {\n      color: #333333;\n      background-color: #FFFFFF;\n      font-size: 100%;\n      font-weight: normal;\n      text-transform: inherit;\n      padding-top: 0;\n      padding-bottom: 0;\n      padding-left: 4px;\n      padding-right: 4px;\n    }\n    \n    #nsgsysbnsb .gt_column_spanner_outer:first-child {\n      padding-left: 0;\n    }\n    \n    #nsgsysbnsb .gt_column_spanner_outer:last-child {\n      padding-right: 0;\n    }\n    \n    #nsgsysbnsb .gt_column_spanner {\n      border-bottom-style: solid;\n      border-bottom-width: 2px;\n      border-bottom-color: #D3D3D3;\n      vertical-align: bottom;\n      padding-top: 5px;\n      padding-bottom: 5px;\n      overflow-x: hidden;\n      display: inline-block;\n      width: 100%;\n    }\n    \n    #nsgsysbnsb .gt_group_heading {\n      padding-top: 8px;\n      padding-bottom: 8px;\n      padding-left: 5px;\n      padding-right: 5px;\n      color: #333333;\n      background-color: #FFFFFF;\n      font-size: 100%;\n      font-weight: initial;\n      text-transform: inherit;\n      border-top-style: solid;\n      border-top-width: 2px;\n      border-top-color: #D3D3D3;\n      border-bottom-style: solid;\n      border-bottom-width: 2px;\n      border-bottom-color: #D3D3D3;\n      border-left-style: none;\n      border-left-width: 1px;\n      border-left-color: #D3D3D3;\n      border-right-style: none;\n      border-right-width: 1px;\n      border-right-color: #D3D3D3;\n      vertical-align: middle;\n      text-align: left;\n    }\n    \n    #nsgsysbnsb .gt_empty_group_heading {\n      padding: 0.5px;\n      color: #333333;\n      background-color: #FFFFFF;\n      font-size: 100%;\n      font-weight: initial;\n      border-top-style: solid;\n      border-top-width: 2px;\n      border-top-color: #D3D3D3;\n      border-bottom-style: solid;\n      border-bottom-width: 2px;\n      border-bottom-color: #D3D3D3;\n      vertical-align: middle;\n    }\n    \n    #nsgsysbnsb .gt_from_md > :first-child {\n      margin-top: 0;\n    }\n    \n    #nsgsysbnsb .gt_from_md > :last-child {\n      margin-bottom: 0;\n    }\n    \n    #nsgsysbnsb .gt_row {\n      padding-top: 8px;\n      padding-bottom: 8px;\n      padding-left: 5px;\n      padding-right: 5px;\n      margin: 10px;\n      border-top-style: solid;\n      border-top-width: 1px;\n      border-top-color: #D3D3D3;\n      border-left-style: none;\n      border-left-width: 1px;\n      border-left-color: #D3D3D3;\n      border-right-style: none;\n      border-right-width: 1px;\n      border-right-color: #D3D3D3;\n      vertical-align: middle;\n      overflow-x: hidden;\n    }\n    \n    #nsgsysbnsb .gt_stub {\n      color: #333333;\n      background-color: #FFFFFF;\n      font-size: 100%;\n      font-weight: initial;\n      text-transform: inherit;\n      border-right-style: solid;\n      border-right-width: 2px;\n      border-right-color: #D3D3D3;\n      padding-left: 5px;\n      padding-right: 5px;\n    }\n    \n    #nsgsysbnsb .gt_stub_row_group {\n      color: #333333;\n      background-color: #FFFFFF;\n      font-size: 100%;\n      font-weight: initial;\n      text-transform: inherit;\n      border-right-style: solid;\n      border-right-width: 2px;\n      border-right-color: #D3D3D3;\n      padding-left: 5px;\n      padding-right: 5px;\n      vertical-align: top;\n    }\n    \n    #nsgsysbnsb .gt_row_group_first td {\n      border-top-width: 2px;\n    }\n    \n    #nsgsysbnsb .gt_summary_row {\n      color: #333333;\n      background-color: #FFFFFF;\n      text-transform: inherit;\n      padding-top: 8px;\n      padding-bottom: 8px;\n      padding-left: 5px;\n      padding-right: 5px;\n    }\n    \n    #nsgsysbnsb .gt_first_summary_row {\n      border-top-style: solid;\n      border-top-color: #D3D3D3;\n    }\n    \n    #nsgsysbnsb .gt_first_summary_row.thick {\n      border-top-width: 2px;\n    }\n    \n    #nsgsysbnsb .gt_last_summary_row {\n      padding-top: 8px;\n      padding-bottom: 8px;\n      padding-left: 5px;\n      padding-right: 5px;\n      border-bottom-style: solid;\n      border-bottom-width: 2px;\n      border-bottom-color: #D3D3D3;\n    }\n    \n    #nsgsysbnsb .gt_grand_summary_row {\n      color: #333333;\n      background-color: #FFFFFF;\n      text-transform: inherit;\n      padding-top: 8px;\n      padding-bottom: 8px;\n      padding-left: 5px;\n      padding-right: 5px;\n    }\n    \n    #nsgsysbnsb .gt_first_grand_summary_row {\n      padding-top: 8px;\n      padding-bottom: 8px;\n      padding-left: 5px;\n      padding-right: 5px;\n      border-top-style: double;\n      border-top-width: 6px;\n      border-top-color: #D3D3D3;\n    }\n    \n    #nsgsysbnsb .gt_striped {\n      background-color: rgba(128, 128, 128, 0.05);\n    }\n    \n    #nsgsysbnsb .gt_table_body {\n      border-top-style: solid;\n      border-top-width: 2px;\n      border-top-color: #D3D3D3;\n      border-bottom-style: solid;\n      border-bottom-width: 2px;\n      border-bottom-color: #D3D3D3;\n    }\n    \n    #nsgsysbnsb .gt_footnotes {\n      color: #333333;\n      background-color: #FFFFFF;\n      border-bottom-style: none;\n      border-bottom-width: 2px;\n      border-bottom-color: #D3D3D3;\n      border-left-style: none;\n      border-left-width: 2px;\n      border-left-color: #D3D3D3;\n      border-right-style: none;\n      border-right-width: 2px;\n      border-right-color: #D3D3D3;\n    }\n    \n    #nsgsysbnsb .gt_footnote {\n      margin: 0px;\n      font-size: 90%;\n      padding-left: 4px;\n      padding-right: 4px;\n      padding-left: 5px;\n      padding-right: 5px;\n    }\n    \n    #nsgsysbnsb .gt_sourcenotes {\n      color: #333333;\n      background-color: #FFFFFF;\n      border-bottom-style: none;\n      border-bottom-width: 2px;\n      border-bottom-color: #D3D3D3;\n      border-left-style: none;\n      border-left-width: 2px;\n      border-left-color: #D3D3D3;\n      border-right-style: none;\n      border-right-width: 2px;\n      border-right-color: #D3D3D3;\n    }\n    \n    #nsgsysbnsb .gt_sourcenote {\n      font-size: 90%;\n      padding-top: 4px;\n      padding-bottom: 4px;\n      padding-left: 5px;\n      padding-right: 5px;\n    }\n    \n    #nsgsysbnsb .gt_left {\n      text-align: left;\n    }\n    \n    #nsgsysbnsb .gt_center {\n      text-align: center;\n    }\n    \n    #nsgsysbnsb .gt_right {\n      text-align: right;\n      font-variant-numeric: tabular-nums;\n    }\n    \n    #nsgsysbnsb .gt_font_normal {\n      font-weight: normal;\n    }\n    \n    #nsgsysbnsb .gt_font_bold {\n      font-weight: bold;\n    }\n    \n    #nsgsysbnsb .gt_font_italic {\n      font-style: italic;\n    }\n    \n    #nsgsysbnsb .gt_super {\n      font-size: 65%;\n    }\n    \n    #nsgsysbnsb .gt_footnote_marks {\n      font-style: italic;\n      font-weight: normal;\n      font-size: 75%;\n      vertical-align: 0.4em;\n    }\n    \n    #nsgsysbnsb .gt_asterisk {\n      font-size: 100%;\n      vertical-align: 0;\n    }\n    \n    #nsgsysbnsb .gt_indent_1 {\n      text-indent: 5px;\n    }\n    \n    #nsgsysbnsb .gt_indent_2 {\n      text-indent: 10px;\n    }\n    \n    #nsgsysbnsb .gt_indent_3 {\n      text-indent: 15px;\n    }\n    \n    #nsgsysbnsb .gt_indent_4 {\n      text-indent: 20px;\n    }\n    \n    #nsgsysbnsb .gt_indent_5 {\n      text-indent: 25px;\n    }\n    </style>\n    <table class=\"gt_table\">\n      \n      <thead class=\"gt_col_headings\">\n        <tr>\n          <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"C\">C</th>\n          <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"kernel\">kernel</th>\n          <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"fold0\">fold0</th>\n          <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"fold1\">fold1</th>\n          <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"fold2\">fold2</th>\n          <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"fold3\">fold3</th>\n          <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"fold4\">fold4</th>\n          <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"cv\">cv</th>\n        </tr>\n      </thead>\n      <tbody class=\"gt_table_body\">\n        <tr><td headers=\"C\" class=\"gt_row gt_right\">1</td>\n    <td headers=\"kernel\" class=\"gt_row gt_left\">linear</td>\n    <td headers=\"fold0\" class=\"gt_row gt_right\">0.947</td>\n    <td headers=\"fold1\" class=\"gt_row gt_right\">0.930</td>\n    <td headers=\"fold2\" class=\"gt_row gt_right\">0.974</td>\n    <td headers=\"fold3\" class=\"gt_row gt_right\">0.921</td>\n    <td headers=\"fold4\" class=\"gt_row gt_right\">0.956</td>\n    <td headers=\"cv\" class=\"gt_row gt_right\">0.946</td></tr>\n        <tr><td headers=\"C\" class=\"gt_row gt_right\">1</td>\n    <td headers=\"kernel\" class=\"gt_row gt_left\">rbf</td>\n    <td headers=\"fold0\" class=\"gt_row gt_right\">0.851</td>\n    <td headers=\"fold1\" class=\"gt_row gt_right\">0.895</td>\n    <td headers=\"fold2\" class=\"gt_row gt_right\">0.930</td>\n    <td headers=\"fold3\" class=\"gt_row gt_right\">0.947</td>\n    <td headers=\"fold4\" class=\"gt_row gt_right\">0.938</td>\n    <td headers=\"cv\" class=\"gt_row gt_right\">0.912</td></tr>\n        <tr><td headers=\"C\" class=\"gt_row gt_right\">10</td>\n    <td headers=\"kernel\" class=\"gt_row gt_left\">linear</td>\n    <td headers=\"fold0\" class=\"gt_row gt_right\">0.939</td>\n    <td headers=\"fold1\" class=\"gt_row gt_right\">0.939</td>\n    <td headers=\"fold2\" class=\"gt_row gt_right\">0.974</td>\n    <td headers=\"fold3\" class=\"gt_row gt_right\">0.947</td>\n    <td headers=\"fold4\" class=\"gt_row gt_right\">0.965</td>\n    <td headers=\"cv\" class=\"gt_row gt_right\">0.953</td></tr>\n        <tr><td headers=\"C\" class=\"gt_row gt_right\">10</td>\n    <td headers=\"kernel\" class=\"gt_row gt_left\">rbf</td>\n    <td headers=\"fold0\" class=\"gt_row gt_right\">0.877</td>\n    <td headers=\"fold1\" class=\"gt_row gt_right\">0.921</td>\n    <td headers=\"fold2\" class=\"gt_row gt_right\">0.912</td>\n    <td headers=\"fold3\" class=\"gt_row gt_right\">0.956</td>\n    <td headers=\"fold4\" class=\"gt_row gt_right\">0.947</td>\n    <td headers=\"cv\" class=\"gt_row gt_right\">0.923</td></tr>\n      </tbody>\n      \n      \n    </table>\n    </div>\n    ```\n    :::\n    :::\n\n\n3.  pick the best combination of hyperparameters[^2]\n\n4.  **REFIT** the model on the WHOLE dataset[^3].\n\n[^2]: presumably based on the average score between folds: this is unclear from the documentation but the numbers match up\n\n[^3]: this is the default, set as False if you do not want the refitting. In the case of multiple metrics, refit should be a string denoting the metric you want to use to find the best estimator.\n\n## Putting the 'tuning' in hyperparameter tuning\n\nIn practice, the data scientist probably does a little more than that. Recall that grid search exhausts the search space we specify. Since we specified two values for both `C` and `kernel`, the size of the grid is merely 4. A continuous search space is infinitely large :-(\n\n### You, the data scientist\n\nYou may not be satisfied with the predictive performance of the model we obtain, and as such you may try to search around the search space for a better hyperparameter value.\n\nSuppose you start with\n\n``` python\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n```\n\nfind out RBF and C = 10 were selected by your search process, then proceed to run\n\n``` python\nparameters = {'kernel': 'rbf', 'C':[9, 10, 11]}\n```\n\nhoping to get better performance.\n\nThe problem is that you are almost certain to get better performance, since you are optimizing for the same cross-validation scheme. We've violated the rule that cross-validation is just for evaluation, and inadvertently used it for optimizing our model!\n\n### Wait ... what just happened?\n\nTo see why this is an issue, let's simplify this problem a little by substituting cross-validation with a validation set approach. Suppose you used a train-validation split on your data and used the test score to guide your hyperparameter choice. And suppose our results looked a little like:\n\n| C   | kernel | test score |\n|-----|--------|------------|\n| 1   | linear | .95        |\n| 1   | rbf    | .91        |\n| 10  | linear | .96        |\n| 10  | rbf    | .90        |\n\nNow you know that 10 and linear are optimal, so you search around the vicinity of C = 10.\n\n| C   | kernel | test score |\n|-----|--------|------------|\n| 8   | linear | .92        |\n| 9   | linear | .93        |\n| 10  | linear | .96        |\n| 11  | linear | .97        |\n\nThis has muddied the train-evaluation distinction. We 'train' our model using our observations for the evaluation set. You'd be hacking the test set for the best score, and the test score loses its efficacy as an indicator of your model's performance. We will choose the hyperparameter that performs best on the test set, but its generalizability is questionable.\n\n### Can cross-validation save us?\n\nUsing cross-validation may not be enough. Suppose we had cross-validation split 1 with the following evaluation scores:\n\n| C   | kernel | cross-validated |\n|-----|--------|-----------------|\n| 1   | linear | .95             |\n| 1   | rbf    | .91             |\n| 10  | linear | .96             |\n| 10  | rbf    | .90             |\n\nand again searched around the vicinity of C = 10:\n\n| C   | kernel | cross-validated |\n|-----|--------|-----------------|\n| 8   | linear | .92             |\n| 9   | linear | .93             |\n| 10  | linear | .96             |\n| 11  | linear | .97             |\n\nyou would be selecting the hyperparameters that are optimal for the cross-validation setup, since the hyperparameter *optimization* process shares the same cross-validation setup as the hyperparameter *evaluation* process. This time, instead of hacking the test set, you're hacking the cross-validation setup.\n\nAs a result, the cross-validation error you obtain from this process would be **overoptimistic**.\n\n## Nested CV\n\nNested CV tries to fix this problem by integrating both training and model selection as part of the model fitting procedure.\n\n[![](images/F1.large.jpg \"Standard nested Cross-Validation (nCV).\")](https://www.biorxiv.org/content/10.1101/2019.12.31.891895v1.full)\n\n@parvandeh2020a\n\nNested CV may seem complicated, but bear with me for a while.\n\n### Vanilla CV\n\nin nested CV, the model is inclusive of the hyperparameter search process, in particular the search space you specify for the hyperparameter search. Instead of cross-validating a set of hyperparameters, we want to benchmark the performance of a hyperparameter search process over K folds of the dataset. This brings us back to the standard cross-validation definition:\n\n![](images/index.png){width=\"628\"}\n\n1.  Train the hyperparameter search process on folds 2-5, then test on fold 1;\n2.  Train the hyperparameter search process on folds 1, 3-5, then test on fold 2;\n3.  Train the hyperparameter search process on folds 1-2, 4-5, then test on fold 3;\n4.  Train the hyperparameter search process on folds 1-3, 5; then test on fold 4;\n5.  Train the hyperparameter search process on folds 1-4; then test on fold 5;\n\nand finally, average the performance on each of the test folds.\n\n### Bringing in hyperparameter search\n\nNow, we would like the hyperparameter search process to be cross-validated as well. So within the hyperparameter search process, we define a different cross-validation configuration. In scikit-learn, this will look like:\n\n\n::: {.cell}\n\n```{.python .cell-code}\np_grid = {\n  \"C\": [1, 10, 100],\n  \"kernel\": [\"rbf\", \"linear\"]\n}\n          \nsvm = SVC()\ninner_cv = KFold(n_splits=4, shuffle=True)\nouter_cv = KFold(n_splits=4, shuffle=True)\nclf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=inner_cv)\nnested_score = cross_val_score(clf, X=X, y=y, cv=outer_cv)\nprint(nested_score)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.95104895 0.98591549 0.96478873 0.93661972]\n```\n:::\n:::\n\n\nAnd once you've found a search space you're happy with, refit the GridSearchCV object:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclf.fit(X, y)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=True),\n             estimator=SVC(),\n             param_grid={&#x27;C&#x27;: [1, 10, 100], &#x27;kernel&#x27;: [&#x27;rbf&#x27;, &#x27;linear&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=KFold(n_splits=4, random_state=None, shuffle=True),\n             estimator=SVC(),\n             param_grid={&#x27;C&#x27;: [1, 10, 100], &#x27;kernel&#x27;: [&#x27;rbf&#x27;, &#x27;linear&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>\n```\n:::\n\n```{.python .cell-code}\nclf.best_params_\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'C': 1, 'kernel': 'linear'}\n```\n:::\n:::\n\n\nWhat is the consequence of this?\n\n1.  **Each of the folds may have a different set of hyperparameters.** This is perfectly fine, as we are now trying to evaluate which hyperparameter search space/process works best, not which set of hyperparameters works best.\n2.  The final test set may have another set of hyperparameters.\n3.  When you eventually retrain on all the data you have, you may get a different set of hyperparameters! But that's okay, because at the end of the day, we're evaluating a process for fitting the model, not a set of hyperparameters.\n\n## What next?\n\n@cawley2010 note that high variance algorithms are more susceptible to overfitting in model selection. This should be troubling for machine learning practitioners, as we like our high-variance algorithms for being able to capture complexity that's missed by traditional high-bias, low-variance algorithms like linear regression.\n\nNested CV can be computationally expensive, and it's difficult to explain to stakeholders why you'd report a development sample score with different hyperparameters for each fold.\n\nSome alternatives include:\n\n1.  regularization\n2.  early stopping\n3.  ensemble modelling\n4.  hyperparameter averaging.\n\nHowever, in the age of increasingly configurable gradient boosting frameworks such as XGBoost and CatBoost, it's hard to see any one of these alternatives being sufficient to curb overfitting in model selection.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}